#!/bin/bash
# docker-entrypoint.sh
# Startup script for CodedSwitch with Ollama

set -e

echo "ğŸš€ Starting CodedSwitch with Local AI..."

# Start Ollama in background
echo "ğŸ–¥ï¸ Starting Ollama service..."
ollama serve &
OLLAMA_PID=$!

# Wait for Ollama to be ready
echo "â³ Waiting for Ollama to be ready..."
sleep 5

# Verify model is available
echo "ğŸ“¦ Verifying Llama 3.1 model..."
if ollama list | grep -q "llama3.1:8b"; then
    echo "âœ… Llama 3.1 model is ready"
else
    echo "âš ï¸ Model not found, downloading..."
    ollama pull llama3.1:8b
fi

# Start CodedSwitch
echo "ğŸµ Starting CodedSwitch server..."
exec node dist/index.cjs
